{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import re\n",
    "import logging;\n",
    "import time\n",
    "import sys\n",
    "import multiprocessing\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = u\"\"\n",
    "book1 = codecs.open('got1.txt', 'r', \"utf-8\")\n",
    "book2 = codecs.open('got2.txt', 'r', \"utf-8\")\n",
    "book3 = codecs.open('got3.txt', 'r', \"utf-8\")\n",
    "book4 = codecs.open('got4.txt', 'r', \"utf-8\")\n",
    "book5 = codecs.open('got5.txt', 'r', \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus += book1.read()\n",
    "corpus += book2.read()\n",
    "corpus += book3.read()\n",
    "corpus += book4.read()\n",
    "corpus += book5.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7173840"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the corpus a bit\n",
    "def sentence_to_wordlist(sentence):\n",
    "    no_tabs = str(sentence).replace('\\t', ' ').replace('\\n', ' ');\n",
    "    \n",
    "        #Remove all characters except A-Z and a dot.\n",
    "    alphas_only = re.sub(\"[^a-zA-Z\\.]\", \" \", no_tabs);\n",
    "        \n",
    "        #Normalize spaces to 1\n",
    "    multi_spaces = re.sub(\" +\", \" \", alphas_only);\n",
    "        \n",
    "        #Strip trailing and leading spaces\n",
    "    no_spaces = multi_spaces.strip();\n",
    "        \n",
    "        #Normalize all charachters to lowercase\n",
    "    clean_text = no_spaces.lower();\n",
    "    words = clean_text.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['copyright', 'by', 'george', 'r.', 'r.', 'martin.']\n",
      "Copyright Â© 1999 by George R. R. Martin.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[3])\n",
    "print(raw_sentences[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-24 12:21:28,568 : INFO : collecting all words and their counts\n",
      "2018-06-24 12:21:28,569 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-06-24 12:21:28,597 : INFO : PROGRESS: at sentence #10000, processed 142262 words, keeping 11815 word types\n",
      "2018-06-24 12:21:28,628 : INFO : PROGRESS: at sentence #20000, processed 282233 words, keeping 15975 word types\n",
      "2018-06-24 12:21:28,662 : INFO : PROGRESS: at sentence #30000, processed 428399 words, keeping 19464 word types\n",
      "2018-06-24 12:21:28,693 : INFO : PROGRESS: at sentence #40000, processed 572939 words, keeping 22085 word types\n",
      "2018-06-24 12:21:28,732 : INFO : PROGRESS: at sentence #50000, processed 715371 words, keeping 24020 word types\n",
      "2018-06-24 12:21:28,767 : INFO : PROGRESS: at sentence #60000, processed 847357 words, keeping 25724 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-24 12:21:28,809 : INFO : PROGRESS: at sentence #70000, processed 981643 words, keeping 27147 word types\n",
      "2018-06-24 12:21:28,850 : INFO : PROGRESS: at sentence #80000, processed 1126788 words, keeping 28470 word types\n",
      "2018-06-24 12:21:28,889 : INFO : PROGRESS: at sentence #90000, processed 1281637 words, keeping 29544 word types\n",
      "2018-06-24 12:21:28,904 : INFO : collected 29978 word types from a corpus of 1348708 raw words and 94294 sentences\n",
      "2018-06-24 12:21:28,905 : INFO : Loading a fresh vocabulary\n",
      "2018-06-24 12:21:28,920 : INFO : min_count=40 retains 2988 unique words (9% of original 29978, drops 26990)\n",
      "2018-06-24 12:21:28,921 : INFO : min_count=40 leaves 1186135 word corpus (87% of original 1348708, drops 162573)\n",
      "2018-06-24 12:21:28,931 : INFO : deleting the raw counts dictionary of 29978 items\n",
      "2018-06-24 12:21:28,933 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2018-06-24 12:21:28,933 : INFO : downsampling leaves estimated 842329 word corpus (71.0% of prior 1186135)\n",
      "2018-06-24 12:21:28,943 : INFO : estimated required memory for 2988 words and 100 dimensions: 3884400 bytes\n",
      "2018-06-24 12:21:28,945 : INFO : resetting layer weights\n",
      "2018-06-24 12:21:28,990 : INFO : training model with 8 workers on 2988 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-06-24 12:21:29,448 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-06-24 12:21:29,449 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-06-24 12:21:29,455 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-06-24 12:21:29,455 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-24 12:21:29,456 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-24 12:21:29,457 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-24 12:21:29,458 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-24 12:21:29,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-24 12:21:29,459 : INFO : EPOCH - 1 : training on 1348708 raw words (841861 effective words) took 0.5s, 1829495 effective words/s\n",
      "2018-06-24 12:21:29,924 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-06-24 12:21:29,926 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-06-24 12:21:29,927 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-06-24 12:21:29,928 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-24 12:21:29,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-24 12:21:29,933 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-24 12:21:29,935 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-24 12:21:29,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-24 12:21:29,936 : INFO : EPOCH - 2 : training on 1348708 raw words (842820 effective words) took 0.5s, 1794926 effective words/s\n",
      "2018-06-24 12:21:30,399 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-06-24 12:21:30,405 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-06-24 12:21:30,408 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-06-24 12:21:30,411 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-24 12:21:30,414 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-24 12:21:30,416 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-24 12:21:30,417 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-24 12:21:30,417 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-24 12:21:30,418 : INFO : EPOCH - 3 : training on 1348708 raw words (842253 effective words) took 0.5s, 1802137 effective words/s\n",
      "2018-06-24 12:21:30,926 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-06-24 12:21:30,927 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-06-24 12:21:30,930 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-06-24 12:21:30,930 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-24 12:21:30,931 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-24 12:21:30,934 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-24 12:21:30,937 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-24 12:21:30,938 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-24 12:21:30,938 : INFO : EPOCH - 4 : training on 1348708 raw words (843112 effective words) took 0.5s, 1643370 effective words/s\n",
      "2018-06-24 12:21:31,388 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-06-24 12:21:31,389 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-06-24 12:21:31,392 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-06-24 12:21:31,393 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-06-24 12:21:31,394 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-06-24 12:21:31,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-06-24 12:21:31,398 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-06-24 12:21:31,399 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-06-24 12:21:31,400 : INFO : EPOCH - 5 : training on 1348708 raw words (842006 effective words) took 0.5s, 1854247 effective words/s\n",
      "2018-06-24 12:21:31,400 : INFO : training on a 6743540 raw words (4212052 effective words) took 2.4s, 1748027 effective words/s\n",
      "2018-06-24 12:21:31,401 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-06-24 12:21:31,421 : INFO : saving Word2Vec object under model_full_reddit, separately None\n",
      "2018-06-24 12:21:31,422 : INFO : not storing attribute vectors_norm\n",
      "2018-06-24 12:21:31,423 : INFO : not storing attribute cum_table\n",
      "2018-06-24 12:21:31,452 : INFO : saved model_full_reddit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 2.8850979804992676 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time();\n",
    "\n",
    "#Set the logging format to get some basic updates.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100;    # Dimensionality of the hidden layer representation\n",
    "min_word_count = 40;   # Minimum word count to keep a word in the vocabulary\n",
    "num_workers = multiprocessing.cpu_count();       # Number of threads to run in parallel set to total number of cpus.\n",
    "context = 5          # Context window size (on each side)                                                       \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model. \n",
    "#The LineSentence object allows us to pass in a file name directly as input to Word2Vec,\n",
    "#instead of having to read it into memory first.\n",
    "\n",
    "print(\"Training model...\");\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling);\n",
    "\n",
    "# We don't plan on training the model any further, so calling \n",
    "# init_sims will make the model more memory efficient by normalizing the vectors in-place.\n",
    "model.init_sims(replace=True);\n",
    "\n",
    "# Save the model\n",
    "model_name = \"model_full_reddit\";\n",
    "model.save(model_name);\n",
    "\n",
    "print('Total time: ' + str((time.time() - start)) + ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/therochvoices/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.04917054, -0.05048471,  0.06292479, -0.05428317, -0.00311557,\n",
       "        0.01370272, -0.15277663, -0.06804673,  0.06142257,  0.15081778,\n",
       "        0.17524628, -0.07025087, -0.06792534, -0.00923483,  0.1412522 ,\n",
       "       -0.07511839,  0.11816825, -0.03708433, -0.05193344, -0.05455773,\n",
       "        0.0112755 , -0.11733751,  0.13758023, -0.0191504 , -0.05128028,\n",
       "        0.06645479, -0.10306124,  0.1413354 ,  0.09387906, -0.12837325,\n",
       "        0.0894337 ,  0.00385344,  0.14094281,  0.01064624, -0.2046606 ,\n",
       "        0.10617352, -0.1031416 , -0.05451788,  0.15164785, -0.08155971,\n",
       "        0.02658188, -0.06194078, -0.14190826,  0.01226823,  0.07333271,\n",
       "       -0.02148726, -0.01249938,  0.0246108 ,  0.01388751, -0.01129342,\n",
       "        0.05825   ,  0.14429316,  0.06563914, -0.1865242 ,  0.07074893,\n",
       "       -0.03166453, -0.06125454,  0.17775598,  0.16565175, -0.02351902,\n",
       "        0.00883297,  0.09091604, -0.20682608,  0.08119093,  0.02524574,\n",
       "       -0.09114547, -0.06399494, -0.03356898, -0.10835277, -0.11040489,\n",
       "       -0.12462556, -0.07534204,  0.07975948,  0.0058813 ,  0.12480994,\n",
       "       -0.25874713, -0.03594804, -0.00381331, -0.20019737,  0.05124164,\n",
       "       -0.01923642, -0.05534529,  0.06639529, -0.13917689,  0.18418607,\n",
       "        0.04774226,  0.24402575,  0.09702732,  0.00897672,  0.20455714,\n",
       "       -0.05366949,  0.09129861, -0.04079735,  0.07715142, -0.02041836,\n",
       "       -0.06631073, -0.03786644, -0.0525776 , -0.00401724,  0.09704002],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = model.wv.syn0;\n",
    "\n",
    "print(Z[0].shape)\n",
    "Z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++')\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 50);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(centroid_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "\n",
    "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "\n",
    "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "\n",
    "#A DataFrame is generated from the dictionary.\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "\n",
    "    return df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_top_words(model.wv.index2word, 2000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cloud(cluster_num, cmap):\n",
    "    wc = WordCloud(background_color=\"black\", max_words=2000, max_font_size=80, colormap=cmap);\n",
    "    wordcloud = wc.generate(' '.join([word for word in top_words['Cluster #' + str(cluster_num).zfill(2)]]))\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('cluster_' + str(cluster_num), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Cluster #01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Cluster #01'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-ee56a56d638b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdisplay_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-6e4de4266db4>\u001b[0m in \u001b[0;36mdisplay_cloud\u001b[0;34m(cluster_num, cmap)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwordcloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Cluster #01'"
     ]
    }
   ],
   "source": [
    "'''cmaps = cycle([\n",
    "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
    "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'hsv',\n",
    "            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])\n",
    "\n",
    "for i in range(20):\n",
    "    col = next(cmaps)\n",
    "    display_cloud(i+1, col)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>man</td>\n",
       "      <td>0.951817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bear</td>\n",
       "      <td>0.853822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crow</td>\n",
       "      <td>0.850769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>giant</td>\n",
       "      <td>0.849724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eye</td>\n",
       "      <td>0.848270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>0.845528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>boy</td>\n",
       "      <td>0.840857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>man.</td>\n",
       "      <td>0.834571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>singer</td>\n",
       "      <td>0.819985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>priest</td>\n",
       "      <td>0.815728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy  similarity\n",
       "0     man    0.951817\n",
       "1    bear    0.853822\n",
       "2    crow    0.850769\n",
       "3   giant    0.849724\n",
       "4     eye    0.848270\n",
       "5    year    0.845528\n",
       "6     boy    0.840857\n",
       "7    man.    0.834571\n",
       "8  singer    0.819985\n",
       "9  priest    0.815728"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_word_table(table, key):\n",
    "    return pd.DataFrame(table, columns=[key, 'similarity'])\n",
    "\n",
    "print_word_table(model.wv.most_similar_cosmul(positive=['king', 'woman'], negative=['queen']), 'Analogy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jon'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"arya sansa jon rickon bran robb\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/therochvoices/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('arryn', 0.8534143567085266),\n",
       " ('brandon', 0.852333664894104),\n",
       " ('eddard', 0.8336315751075745),\n",
       " ('stark.', 0.8271368741989136),\n",
       " ('murdered', 0.7937789559364319),\n",
       " ('lysa', 0.7864471077919006),\n",
       " ('daenerys', 0.7388279438018799),\n",
       " ('ward', 0.7352973818778992),\n",
       " ('tully', 0.7342249155044556),\n",
       " ('greyjoy', 0.7277718186378479)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"stark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = model.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stark is related to winterfell, as tully is related to riverrun\n",
      "jaime is related to sword, as arya is related to wine\n",
      "arya is related to nymeria, as words is related to dragons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/therochvoices/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar_cosmul` (Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'words'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_similarity_cosmul(\"stark\", \"winterfell\", \"riverrun\")\n",
    "nearest_similarity_cosmul(\"jaime\", \"sword\", \"wine\")\n",
    "nearest_similarity_cosmul(\"arya\", \"nymeria\", \"dragons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
